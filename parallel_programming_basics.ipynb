{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Some important points: \\\n",
        "Speedup = T(1) / T(p) -> p is the number of compute units used. \\\n",
        "Efficiency = T(1) / (p * T(p)). \\\n",
        "if computation time is $\\alpha$ and communication time is $\\beta$, and computation-communication time is given by $\\gamma$, the efficieny drops with decrease in $\\gamma$. more the $\\gamma$, more the efficiency and speedup and also with more compute units p, there is an increase in efficiency till a global maximum after which the communication time starts dominating and reduces efficiency."
      ],
      "metadata": {
        "id": "txhx0zSGn1AO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributed memory systems: \\\n",
        "In some parallel algorithms like above, each PE has access to its own local memory and if it needs to communicate with other PEs, it has to be explicitly done through an interconnection network. Remote data access needs to be facilitated through message passing over interconnection network. \\\n",
        "The interconnection network is an important architectural factor of a distributed memory system. \\\n",
        "Standard network protocols (such as Infiniband or Ethernet) are often used to implement the communication tasks. \\\n",
        "Network topology : determines the scalability of the architecuture for many applications. \\\n",
        "Prominent examples of distributed memory systems are compute clusters and network-on-chip (NOC) architectures. \\\n",
        "The message passing interface (MPI) is arguably the most popular language for parallel programming on distributed memory systems. \\\n",
        "Data exchanges between two processes can be implemented using (versions of) MPI_Send and MPI_Recv commands while data communication between groups of processes can be implemented by collective communication functions such as MPI_Bcast, MPI_Reduce, MPI_Gather, or MPI_Scatter. \\\n",
        "Data partitioning i.e distribution of data between the processes is a key issue in distributed memory systems. \\\n",
        "Partitioned Global Address Space (PGAS) is another popular approach to develop programs for distributed memory systems. \\\n",
        "The PGAS model is the basis of UPC++ .\n",
        "\n"
      ],
      "metadata": {
        "id": "-fEgb3D6w1Em"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shared Memory systems : \\\n",
        "All CPUs (or cores) can access a common memory space through a shared bus or crossbar switch. \\\n",
        "Prominent examples of such systems are modern multi-core CPU-based workstations in which all cores share the same main memory. \\\n",
        "In addition to the shared main memory each core typically also contains a smaller local memory (e.g. Level 1 cache) in order to reduce expensive accesses to main memory (known as the von Neumann bottleneck). \\\n",
        "Cache coherence : n order to guarantee correctness, values stored in (writable) local caches must be coherent with the values stored in shared memory. \\\n",
        "Modern multi-core systems support cache coherence and are often also referred to as cache coherent non-uniform access architectures (ccNUMA) \\\n",
        "Parallelism is typically created by starting threads running concurrently on the system. Exchange of data is usually implemented by threads reading from and writing to shared memory locations. \\\n",
        "race conditions should be avoided. \\\n",
        "A race condition can occur when two threads access a shared variable simultaneously (without any locking or synchronization), which could lead to unexpected results. \\\n",
        "A number of programming techniques (such as mutexes, condition variables, atomics), which can be used to avoid race conditions. \\\n",
        "A program typically starts with one process running a single thread. This master thread creates a number of slave threads which later join the master thread in order to terminate. Each thread can define its own local variables but has also access to shared variables. Thread creation is much more lightweight and faster compared to process creation. Therefore threads are often dynamically created and terminated during program execution. \\\n",
        "OpenMP is another approach to multi-threaded programming (Chapter 6) based on semiautomatic parallelization. \\\n",
        "They simplify parallelization through APIs which operate through pragmas. Pragmas are prep-processor directives to compiler which can be used to generate multi-threaded code. \\\n",
        "Thus, when parallelizing a sequential code with OpenMP, a programmer often only needs to annotate the code with the suitable pragmas. Nevertheless, achieving a highly efficient and scalable implementation can still require in-depth knowledge. \\\n",
        "\n",
        "The utilized number of threads in a program can range from a small number (e.g., using one or two threads per core on a multi-core CPU) to thousands or even millions. This type of massive multi- threading is used on modern accelerator architectures\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P1_UHufzsXK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerations when designing parallel algorithms : \\\n",
        "Partitioning : The given project can be parallelize in different ways among the processing elements [cores] , task parallelism, model parallelism, data parallelism. \\\n",
        "Communication : the partition scheme depends on the amount and types of communication required between processes or threads. \\\n",
        "Synchronization : In order for parallel computations to work properly, the threads / processes need to be synchronized. \\\n",
        "Load balancing : The total work needs to be equally distributed among available threads or processes to not overburden any one thread/process. \\\n"
      ],
      "metadata": {
        "id": "O3E0AXQVBCn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding parallelism in loop carried data dependency is difficult but not impossible, with careful study of data in han, it can be achieved with some clever tricks. Consider a for loop: \\\n",
        "for(int i=0;i<N; i++) A[i] = A[i-1] + A[i] \\\n",
        "let the array be : [6,3,5,8,1,8,0,2,4,3,5,2,1,7,4,1,2,3,5,1] of 20 elements.\n",
        "let the array be divided into 4 cores with 5 elements each. \\\n",
        "perform the same computations on each of the cores will give: \\\n",
        "PE 1: [6,9,14,22,23] \\\n",
        "PE 2: [8,8,10,14,17] \\\n",
        "PE 3: [5,7,8,15,19] \\\n",
        "PE 4 : [1,3,6,11,12] \\\n",
        "\n",
        "These four PE's sums need to be synced. \\\n",
        "the last element of PE 1, can be taken and added with all elements in PE2, So, elements in PE 2 become : \\\n",
        "PE 2: [31,31,33,37,40] \\\n",
        "at the same time, the last element of PE 3 can be added to PE4 : \\\n",
        "PE 4 : [20,2,25,30,31].\n",
        "\n",
        "The final step is to add PE2's last element  to PE 3 and PE 4: \\\n",
        "PE 3 : [45,47,48,55,59]\n",
        "PE 4 : [60,62,65,70,71]"
      ],
      "metadata": {
        "id": "U0Bmclh_IJ6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_list= [6,3,5,8,1,8,0,2,4,3,5,2,1,7,4,1,2,3,5,1]\n",
        "new_list = []\n",
        "new_list.append(my_list[0])\n",
        "for i in range(1,len(my_list)):\n",
        "    new_list.append(my_list[i]+new_list[i-1])\n",
        "print(new_list)\n"
      ],
      "metadata": {
        "id": "GEQ3n-e8wy5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7075e65-356e-477e-8ac5-a206a141b1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 9, 14, 22, 23, 31, 31, 33, 37, 40, 45, 47, 48, 55, 59, 60, 62, 65, 70, 71]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the above python code matches with our analysis, the difference being we are able to data parallelize this with some additional transfers required in between"
      ],
      "metadata": {
        "id": "MqqgfqrKlLwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choice of partitioning strategy is crucial for parallel algorithm design.\\ Data parallelism distributes the data across different processors or cores\n",
        "which can then operate on their assigned data. \\\n",
        "Some data parallel\n",
        "algorithms are even embarrassingly parallel and can operate independently on their assigned data; e.g.,\n",
        "in an image classification task different images can be assigned to different processors which can then\n",
        "classify each image independently in parallel. \\\n",
        "For the implementation of a data parallel algorithm you sometimes need to perform synchronization\n",
        "between processes or threads. \\\n",
        "Task parallelism : \\\n",
        "Consider a ternary classifier, where each image passed through three binary classifiers and then the results are merged to get the result. \\\n",
        "Data parallelism would have this entire model on each core / PE, and different images are put through these cores giving out results. \\\n",
        "Task parallelism would mean having these three classifiers on three different cores / PE -> P0,P1,P2 and then merging is done on p0 for each image. \\\n",
        "To scale towards large number of processors, task parallelism and data parallism will be combined together. \\"
      ],
      "metadata": {
        "id": "vZWhOYxn5l5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "when multiple PEs process data, one of the operations in task parallelism could be expensive and could take more time to learn. for example : human classification. This might limit the achievable speedup because other two PEs are remaining idle. Load balancing makes sure that speedup is not reduced. \\\n",
        "In data parallel approach, the input image dataset is divided into batches and the images are fed batch by batch into the PE. Once a Process has completed its batch, a dynamic scheduler assigns a new batch to the idle PE."
      ],
      "metadata": {
        "id": "6WR1zifJ_2wV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to train neural networks, the images are fed to massively parallel GPUs which process apply matrix operations on these images and churn out outputs. Some neural networks are complex and their size (weights) can exceed the main memory of the GPU. So , the usual data-parallel approach does not work. The model itself is broken down into chunks and fed to individual computing components of the GPU. The weights are equally distributed among GPUs and each GPU does the matrix operation associated with that weight alone. the vector produced by each of the GPU after each layer need to be aligned before proceeding with the next layer."
      ],
      "metadata": {
        "id": "fR6yXjV2BxLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High Performance Computers [HPC] : \\\n",
        "Most of them use neo-heteregeneous architecture containing millions of cores along with CUDA-based GPU accelerators which churn out billions of FLOPS/sec. \\\n",
        "They also employ different levels of parallelism : \\\n",
        "Node level parallelism : requires implementation of algorithms for distributed memory model using MPI .\\\n",
        "Intra-node parallelization : requires libraries which implement parallelism with shared memory [OpenMP] , [multithreading]. \\\n",
        "Accelerator-level parallelization : offloads some of the computations to dedicated accelerators like GPUs for parallization."
      ],
      "metadata": {
        "id": "hKwflnSy2okY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Exercises: \\\n",
        "1.Analyze the speedup and the efficiency of the parallel summation algorithm presented in Section 1.1 using n = 2048 numbers assuming that each PE can add two numbers in one millisecond and each PE can send m numbers in 2 + m/1024 milliseconds to another PE. Vary the number of PEs from 1 to 1024 using powers of 2. \\\n",
        "Soln : n = 2048.\\\n",
        "let us consider the simpler case, where number of PE is 1. [the base case]\n",
        "T(1) = n - 1 = 2047 ms. \\\n",
        "Speedup = T(1) / T(n) = T(1) / T(1) = 1 \\\n",
        "p = 2 ,[number of PEs = 2] \\\n",
        "the numbers can be split up into two PEs each. to transfer 2048/2 = 1024 numbers to PE1 from PE0, the time taken is 2 + 1024/1024 = 3 ms. \\\n",
        "The computation time is 1024 - 1 = 1023 ms in both PEs. to bring back the sum of 1024 elements to PE0, the time taken in 2 + 1/1024 = 2.0009765625 ms. \\\n",
        "To sum the two computed sums, the time taken is 1 ms.\\\n",
        "Total time taken = 3 + 2.001 + 1023 + 1 = 1029.001 ms. \\\n",
        " Speedup = T(1) / T(2) = 2047 / 1029.001 = 1.989, efficiency = T(1) / (2*T(2)) = 0.994. \\\n",
        "\n",
        " p = 2^2 = 4 : \\\n",
        " PE 0 sends half its inputs to PE 1 in 2 + 1024/1024 = 3 ms. PE0 and then half of its current inputs to PE2 and PE1 can send half of its inputs to PE3 in 2 + 512/1024 = 2.5 ms. \\\n",
        " Computation of 512-1 additions in 511 ms . PE1 sends sum to PE0 and PE3 send sum to PE 2 in 2+1/1024 ~ 2 ms. One addition in PE0 and PE 2 = 1 ms. Sending of data in PE 2 to PE 0 is ~ 2 ms and final addition is of time 1 ms. \\\n",
        " total sum = 3 + 2.5 + 511 + 2+ 1+ 2+ 1 = 522.5 ms. \\\n",
        " Speedup = T(1)/T(4) = 2047 / 522.5 = 3.917, efficiency = 0.9794. \\\n",
        "\n",
        " p = 2^3 = 8 : \\\n",
        " PE0 send half its inputs to PE 1 -> 2 + 1024/1024 = 3 ms. PE 0 and PE 1 send half of existing inputs to PE 2 and PE 3 in 2 + 512/1024 = 2.5 ms. PEs 0 , 1, 2, 3 send half of existing inputs to PE4, PE5, PE6, PE7 in 2 + 256/1024 = 2.25 ms. Addition time : 256-1 = 255 ms. Sum from PE7 can be transferred to PE3, from PE6 to PE 2, PE5 to PE1 and PE4 to PE 0 in 2 + 1/2024 = 2.0001 ms. the four addition operations can be done in 1 ms . \\\n",
        " Sum from PE4 can be taken to PE3 can be taken to PE1 and PE 2 to PE 0 in 2.0001 ms. 1 ms for addition and one final movement to PE0 from PE 1 in 2.0001 ms and one final addition in 1 ms. \\\n",
        " total time = 3 + 2.5 + 2.25 + 255 + 2.0001 + 1 + 2.0001 + 1 + 2.0001 + 1 = 271.7503 ms. speedup = 7.53265037794, efficieny = 7.53265037794/8 = 0.94158. \\\n",
        "\n",
        " General trend : \\\n",
        " Distribution of inputs among p cores : \\\n",
        " let p = 2^m, then distribution of inputs to 2^m cores takes $2 \\times m + \\sum _{i=1}^{m} 1/m$. \\\n",
        " time for addition : \\\n",
        " $T_{add} = \\frac{n}{p} - 1$ \\\n",
        " Time for recollection : \\\n",
        " $T_{recollection} $= (2.0001 + 1) * m = 3.0001m ~ 3m$. \\\n",
        "\n",
        " Now, for n = 2048, p = 1024, m = 10. \\\n",
        " Distribution time = 2 * 10 + (1 + 1/2 + 1/3 + 1/4 + 1/5 + 1/6 + 1/7 + ... 1/10) = 20 + 2.9289 = 22.9289. \\\n",
        " Addition time = 2048/1024 - 1 = 1 ms \\\n",
        " Collection time = 3 * 10 = 30 ms. \\\n",
        "Total time = 22.93 + 30 + 1 = 53.93 ms. \\\n",
        "Speedup = 2047 / 53.98 = 37.956, efficieny = 0.0370 -> due to communication time, there is a drop in efficieny with 1024 cores.\n"
      ],
      "metadata": {
        "id": "Wny3QcEQ3m4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Consider the parallel prefix computation algorithm presented in Fig. 1.9. Describe how this parallel algorithm works in general on a shared memory machine using an input array of size n = $2^k$ and n/4 cores. How high is the achieved speedup? \\\n",
        "Soln : it is a shared memory system. On a single core, the prefix computation would take n - 1 time units  = $2^k$ - 1 time units. \\\n",
        "With n/4 cores, 4 numbers can be assigned to each core, meaning the time taken would be 4 - 1 time-units. Now the sums need to be synchronized. \\\n",
        "You will have n/4 numbers local sub-array last numbers which will be stored in another array. Prefix computation will be done in these numbers. this can be done in parallel to in $\\log _2 {n/4}$ = k-2 time units. \\\n",
        "the remaining operation is to add these last numbers of local sub-arrays to next sub-array. that can be done in a vectorized way as wall thereby parallelizing the operation to be done in 1 time unit. \\\n",
        "Total time will thus be : 3 + k - 2 + 3 [if sequential or 1 if vectorized] = k + 4 units [assuming sequential addition] .\n",
        "T(1) = 2^k - 1. \\\n",
        "Speedup = $\\frac{2^k-1}{k+4}$"
      ],
      "metadata": {
        "id": "32ZSKqdaJEFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Histogram problem : \\\n",
        "The computation of a histogram is a frequent operation in image processing. The histogram simply counts the number of occurrences of each tonal value in the given image. Consider a 2-dimensional input gray-scale image I of size n × n.\n",
        "\n",
        "        for (i=0; i<n; i++)\n",
        "           for (j=0; j<n; j++)\n",
        "               histogram[I[i,j]]++\n",
        "\n",
        "Discuss the advantages and disadvantages of the two following partitioning strategies for computing an image histogram in parallel:\n",
        "a. The histogram slots are partitioned between processors.\n",
        "b. The input image is partitioned between processors. \\\n",
        "\n",
        "Soln : Baiscally we have an input array I of size n x n. Also output array ,histogram of size 256 [assuming 8 bit grayscale image wit value between 0-255]. if we sequentially compute the operations, we need to go through the entire n x n, update that index of histrogram which is given by the intensity of the image.\n",
        "Case a. histogram slots are partitioned between processors : \\\n",
        "Advantages : \\\n",
        "Reduced conflicts : By assigning a portion of histrogram slots to be updated to different processors, we can prevent conflicts during write operations. The write operation is independent. \\\n",
        "Load balancing : for a image with a uniform distribution of intensity, every processor will be equally utilized. there is good load balancing in most scenarios \\\n",
        "Scalaibility : can be scaled to higher number of processors because each processor handles the assigned subset of histogram slots. \\\n",
        "Disadvantages : \\\n",
        "Global synchronization : The master core has to wait till all cores have done their job in filling the histogram values and combine them to form the final completed global histogram. \\\n",
        "Memory usage : each core has to have its dedicated share of histogram slots potentially leading to memory usage issues if the histogram is large. \\\n",
        "Imbalance in usage of cores : During cases where there is uneven tonal intensity distributions in images, there is imbalance in core usage.\n",
        "\n",
        "Case b . The input image is partitioned between processors \\\n",
        "Advantages : \\\n",
        "Cache utilization : Potentially better usage of cache since chunk of image stored in contiguous blocks can be used with reduced memory access latency. \\\n",
        "Local processing : Each image block is independent of one another and the cores can read them and process the subsequent updation independently.\n",
        "No Global sychronization  :there is just one single buffer of histgrams updated in a core and there is one final synchronization required.\n",
        "\n",
        "Disadvantages : \\\n",
        "Load imbalancing : some patches of image may involve more computations than other patches potentially slowing down certain cores. For example, the edges may not have very uniform dostribution whereas center pixels may have distributed tonacy.  \\\n",
        "Write conflicts : Same index of output histogram slots may have to be filled by multiple cores handling different portion of image and thus we need to enforce locks for dat to prevent data race conditions. \\\n",
        "Complexity in combining results : There are multiple checks to be done so that same data is no improperly updated by different cores, there is some difficulty in combining the tonacy of each individual index in histogram"
      ],
      "metadata": {
        "id": "9PVNukyFqDhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write down all the prime numbers between 1 and 1000 on the board. At each step, you are allowed\n",
        "to erase two numbers on the board (say, x and y) and in place of the two erased numbers write the number x + y + x · y. Repeat this process over and over until only a single number remains (call it Q). Over all possible combination of numbers, what is the smallest value of Q? Assume we have already precomputed the n prime numbers between 1 and 1000. Moreover, we use a third- party library for arbitrary long integers1 and thus do not need to worry about potential overflow of integers.\n",
        "(i) Prove that the operation x ⊙ y := x + y + x · y is commutative and associative.\n",
        "(ii) Using the result of (i), how can we efficiently parallelize this algorithm?\n",
        "(iii) Investigate the runtime of the algorithm on p processor. Discuss the result. \\\n"
      ],
      "metadata": {
        "id": "ha60kkVozjY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#python program to list all prime numbers between 1 and 1000.\n",
        "\n",
        "def sieve_of_eratosthenes(max_num):\n",
        "    \"\"\"Return a list of all prime numbers up to max_num.\"\"\"\n",
        "    is_prime = [True] * (max_num + 1)  # Initialize a boolean list\n",
        "    is_prime[0] = is_prime[1] = False  # 0 and 1 are not prime numbers\n",
        "\n",
        "    for start in range(2, int(max_num**0.5) + 1):\n",
        "        if is_prime[start]:\n",
        "            for multiple in range(start*start, max_num + 1, start):\n",
        "                is_prime[multiple] = False\n",
        "\n",
        "    return [num for num, prime in enumerate(is_prime) if prime]\n",
        "\n",
        "# Define the range\n",
        "max_number = 1000\n",
        "\n",
        "# Get all prime numbers up to max_number\n",
        "primes = sieve_of_eratosthenes(max_number)\n",
        "\n",
        "# Print the list of prime numbers\n",
        "print(f\"Prime numbers between 1 and {max_number}:\")\n",
        "print(primes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90jaKFruz2xc",
        "outputId": "a4cb82e4-932d-48d4-c2b0-3999925158dc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prime numbers between 1 and 1000:\n",
            "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(primes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77ER1acU0M49",
        "outputId": "213af52d-41f5-4748-98ab-43007d4bcd48"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "168"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "there are 168 primes between 1 and 1000 and they have been listed above and arranged in ascending order. let us first erase the numbers pairwise in sequence and replace them with x + y + x.y as required in the problem."
      ],
      "metadata": {
        "id": "2tmtQQ1O0OuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_primes1 = primes.copy()\n"
      ],
      "metadata": {
        "id": "kPtXZCY91XtU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_primes1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azsY7Cs11gNx",
        "outputId": "0d65f99a-264e-4b95-8f9b-9c72053c2fc6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while(1):\n",
        "  if len(new_primes1) == 1:\n",
        "    break\n",
        "  a = new_primes1[0]\n",
        "  b = new_primes1[1]\n",
        "  c = a + b + a*b # also a prime\n",
        "  new_primes1.pop(0)\n",
        "  new_primes1.pop(0)\n",
        "  new_primes1.insert(0,c)\n",
        "\n",
        "print(new_primes1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5BQ2M3U1i-N",
        "outputId": "f9061eb0-6f00-455e-fb74-9823e68b1d16"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11670547439198173018840584368947199999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_primes1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1KoI-IQ13HJ",
        "outputId": "6a5edb39-141a-49fb-990c-2562ac5b199b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this sequential way of taking two numbers and replacing it with third number which is x+y+x*y gives a very large number. Q is an insanely large number and is definitely not the lowest achievable. let us try anoher approach.let us combine first and last of the list at each iteration and check what we get again."
      ],
      "metadata": {
        "id": "pTYQLJiJ2GS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_primes2 = primes.copy()\n",
        "while(1):\n",
        "  if len(new_primes2) == 1:\n",
        "    break\n",
        "  a = new_primes2[0]\n",
        "  b = new_primes2[-1]\n",
        "  c = a + b + a*b # also a prime\n",
        "  new_primes2.pop(0)\n",
        "  new_primes2.pop(-1)\n",
        "  new_primes2.insert(-1,c)\n",
        "\n",
        "print(new_primes2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odjgPIQV2CCg",
        "outputId": "05c198a1-137a-4fb5-f72a-c0f5f2571e43"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11670547439198173018840584368947199999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_primes1[0] - new_primes2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNJWYjgI25c2",
        "outputId": "b6d3baa8-bff1-4a11-88e2-4d383e72b82f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "W7vfN2mSCOEb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the approaches give the same results. the final Q value is the same."
      ],
      "metadata": {
        "id": "2NixgnQa29CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sieve_of_eratosthenes(max_num):\n",
        "    \"\"\"Return a list of all prime numbers up to max_num.\"\"\"\n",
        "    is_prime = [True] * (max_num + 1)\n",
        "    is_prime[0] = is_prime[1] = False\n",
        "\n",
        "    for start in range(2, int(max_num**0.5) + 1):\n",
        "        if is_prime[start]:\n",
        "            for multiple in range(start*start, max_num + 1, start):\n",
        "                is_prime[multiple] = False\n",
        "\n",
        "    return [num for num, prime in enumerate(is_prime) if prime]\n",
        "\n",
        "def smallest_final_value(max_num):\n",
        "    # Get all primes between 1 and max_num\n",
        "    primes = sieve_of_eratosthenes(max_num)\n",
        "\n",
        "    # Compute the product of (p_i + 1) for each prime p_i\n",
        "    product = 1\n",
        "    for prime in primes:\n",
        "        product *= (prime + 1)\n",
        "\n",
        "    # The result is this product minus 1\n",
        "    return product - 1\n",
        "\n",
        "# Define the range\n",
        "max_number = 1000\n",
        "\n",
        "# Find the smallest final value\n",
        "start_time = time.time()\n",
        "result = smallest_final_value(max_number)\n",
        "execution_time = time.time() - start_time\n",
        "\n",
        "print(\"execution time without parallelization : \", execution_time)\n",
        "\n",
        "# Print the result\n",
        "print(f\"The smallest value of Q is: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwFoRppP28Ci",
        "outputId": "4949c4b1-8b49-443f-b53e-e7eb5bc1ea3d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "execution time without parallelization :  0.0020780563354492188\n",
            "The smallest value of Q is: 147112613210319486499676296015469376140521397487772312539818224915030479615918413181667804881965140978577602879012396476181147114517095334951755284774296929590260545569265720366451220763460675369225562092691997648611896298415668747092796050814927021662304746378459453311470775199933251758665556263651853490184370195055575968818253407088121287627256915970363971231352356863999999999999999999999999999999999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result - new_primes1[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSNL-Rhl3yl-",
        "outputId": "98422dad-3599-494f-8a8d-e6c52e9506d1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No matter which two numbers you take and do this continuously, you get the same value of Q. At the end, x+y+x.y = (x+1). (y+1) - 1. So, we need to find the product of all primes + 1 and subtract from 1 in the end."
      ],
      "metadata": {
        "id": "Y0GxoRHt37E5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commutativity : x+y+x.y\n",
        "if we replace x with y and y with x -> y+x+y.x which is same as original expression, meaning the operation is commutative. \\\n",
        "Associativity : \\\n",
        "(x+y+x.y) + z + (x+y+x.y)*z = x + y + z + x.y + z.x + z.y + x.y.z , if we now combine y and z first and then do this operation with x , we get : \\\n",
        "y+z+y.z + x + (y+z+y.z)*x = y+z+x+y.z+x.y+x.z+x.y.z ,which is same as above expression hence it is associate, thus this concludes that the order in which we do this till we are left with one number does not matter, it is going to reach the same result. \\\n",
        "We need to add 1 to every prime number and compute the product. \\\n",
        "This can be parallelized. \\\n",
        "for a single processor, first we add one to every number - n operations. \\\n",
        "then we iterate through every element and keep computing products , so it is still n - 1 multiplications. finally, we subtract 1 from the answer. \\\n",
        "In case of parallelization: \\\n",
        "we have 168 numbers, let us say we have 8 cores, we shift 84 numbers to core 2, and then half of the existing inputs to core 3 and 4 and so on , till we have number equally distributed among 8 cores, 21 numbers per core. \\\n",
        "then we can add 1 to each number parallely using vectorization, load four numbers into the vector , add 1 and then load it back to memory. \\\n",
        "product can be computed per core and the product can be saved at the last location. once we have products for all 8 cores, we then do prefix product like we did for sum before in log2 times. finally we can take the product and subtract from 1 to get the result.\n"
      ],
      "metadata": {
        "id": "F7cXD9CE4Q-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here is the python implementation of the above idea of parallelization using threads. [chatgpt helped me with the implementation to be honest!]"
      ],
      "metadata": {
        "id": "NVt0Tqfo8AnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "from functools import reduce\n",
        "from operator import mul\n",
        "\n",
        "def sieve_of_eratosthenes(max_num):\n",
        "    \"\"\"Return a list of all prime numbers up to max_num.\"\"\"\n",
        "    is_prime = [True] * (max_num + 1)\n",
        "    is_prime[0] = is_prime[1] = False\n",
        "\n",
        "    for start in range(2, int(max_num**0.5) + 1):\n",
        "        if is_prime[start]:\n",
        "            for multiple in range(start*start, max_num + 1, start):\n",
        "                is_prime[multiple] = False\n",
        "\n",
        "    return [num for num, prime in enumerate(is_prime) if prime]\n",
        "\n",
        "def parallel_product(numbers):\n",
        "    \"\"\"Compute the product of a list of numbers in parallel.\"\"\"\n",
        "    def worker(chunk):\n",
        "        return reduce(mul, chunk, 1)\n",
        "\n",
        "    num_threads = min(len(numbers), 2)  # Number of parallel workers (adjust based on your system)\n",
        "    chunk_size = len(numbers) // num_threads\n",
        "    chunks = [numbers[i:i + chunk_size] for i in range(0, len(numbers), chunk_size)]\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "        results = list(executor.map(worker, chunks))\n",
        "\n",
        "    return reduce(mul, results, 1)\n",
        "\n",
        "def smallest_final_value(max_num):\n",
        "    # Get all primes between 1 and max_num\n",
        "    primes = sieve_of_eratosthenes(max_num)\n",
        "\n",
        "    # Compute the product of (p_i + 1) for each prime p_i in parallel\n",
        "    transformed_numbers = [p + 1 for p in primes]\n",
        "    product = parallel_product(transformed_numbers)\n",
        "\n",
        "    # The result is this product minus 1\n",
        "    return product - 1\n",
        "\n",
        "# Define the range\n",
        "max_number = 1000\n",
        "\n",
        "# Find the smallest final value\n",
        "start_time= time.time()\n",
        "result = smallest_final_value(max_number)\n",
        "execution_time = time.time() - start_time\n",
        "\n",
        "print(\"execution time with parallelization : \", execution_time)\n",
        "\n",
        "# Print the result\n",
        "print(f\"The smallest value of Q is: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxJ61q6930ye",
        "outputId": "b3051086-d316-442b-bcec-84884e3b9067"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "execution time with parallelization :  0.0008478164672851562\n",
            "The smallest value of Q is: 147112613210319486499676296015469376140521397487772312539818224915030479615918413181667804881965140978577602879012396476181147114517095334951755284774296929590260545569265720366451220763460675369225562092691997648611896298415668747092796050814927021662304746378459453311470775199933251758665556263651853490184370195055575968818253407088121287627256915970363971231352356863999999999999999999999999999999999999999999999\n"
          ]
        }
      ]
    }
  ]
}